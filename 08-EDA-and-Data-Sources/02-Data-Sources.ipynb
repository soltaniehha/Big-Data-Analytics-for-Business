{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Data Sources\nThis notebook formally introduces the variety of other data sources that you can use with Spark out of the box as well as the countless other sources built by the greater community. Spark has six \u201ccore\u201d data sources and hundreds of external data sources written by the community. The ability to read and write from all different kinds of data sources and for the community to create its own contributions is arguably one of Spark\u2019s greatest strengths. Following are Spark\u2019s core data sources:\n\n* CSV\n* JSON\n* Parquet\n* ORC\n* JDBC/ODBC connections\n* Plain-text files\n\nAs mentioned, Spark has numerous community-created data sources. Here\u2019s just a small sample:\n\n* BigQuery\n* Cassandra\n* HBase\n* MongoDB\n* AWS Redshift\n* XML\n* And many, many others\n\nThe goal of this notebook is to give you the ability to read and write from Spark\u2019s core data sources and know enough to understand what you should look for when integrating with third-party data sources. To achieve this, we will focus on the core concepts that you need to be able to recognize and understand.\n\n## The Structure of the Data Sources API\nBefore proceeding with how to read and write from certain formats, let\u2019s visit the overall organizational structure of the data source APIs.\n\n### Read API Structure\nThe core structure for reading data is as follows:\n\n> ```\nDataFrameReader.format(...).option(\"key\", \"value\").schema(...).load()\n```\n\nWe will use this format to read from all of our data sources. format is optional because by default Spark will use the Parquet format. option allows you to set key-value configurations to parameterize how you will read data. Lastly, schema is optional if the data source provides a schema or if you intend to use schema inference. Naturally, there are some required options for each format, which we will discuss when we look at each format.\n\n### Basics of Reading Data\nThe foundation for reading data in Spark is the DataFrameReader. We access this through the SparkSession via the read attribute:\n\n> ```python\nspark.read\n```\n\nAfter we have a DataFrame reader, we specify several values:\n\n* The format\n* The schema\n* The read mode\n* A series of options\n\nThe format, options, and schema each return a DataFrameReader that can undergo further transformations and are all optional, except for one option. Each data source has a specific set of options that determine how the data is read into Spark (we cover these options shortly). At a minimum, you must supply the DataFrameReader a path to from which to read.\n\nHere\u2019s an example of the overall layout:\n\n``` python\nspark.read.format(\"csv\")\\\n  .option(\"mode\", \"FAILFAST\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"path\", \"path/to/file(s)\")\\\n  .load()\n```\n  \nAs we have seen before we can put the \"path\" option directly in the `load()` method, i.e.: `.load(\"path/to/file(s)\")`\n\n**READ MODES**\nReading data from an external source naturally entails encountering malformed data, especially when working with only semi-structured data sources. Read modes specify what will happen when Spark does come across malformed records. Table below lists the read modes:\n\n\n|Read mode | Description |\n| -- | -- |\n| permissive | Sets all fields to null when it encounters a corrupted record and places all corrupted records in a string column called \\_corrupt_record |\n| dropMalformed | Drops the row that contains malformed records |\n| failFast | Fails immediately upon encountering malformed records |\n\nThe default is *permissive*.\n\n**Example:** Let's load a sample CSV file, similar to what we have seen before:"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "data = \"gs://is843/notebooks/jupyter/data/\"  # Update the bucket name with your bucket\n\ndf = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(data + \"flight-data/csv/2010-summary.csv\")"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 2:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df.show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Write API Structure\nThe core structure for writing data is as follows:\n\n> ```\nDataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...)\\\n  .sortBy(...).save()```\n\nWe will use this format to write to all of our data sources. format is optional because by default, Spark will use the parquet format. option, again, allows us to configure how to write out our given data. *PartitionBy*, *bucketBy*, and *sortBy* work only for file-based data sources; you can use them to control the specific layout of files at the destination.\n\n### Basics of Writing Data\nThe foundation for writing data is quite similar to that of reading data. Instead of the *DataFrameReader*, we have the *DataFrameWriter*. Because we always need to write out some given data source, we access the *DataFrameWriter* on a per-DataFrame basis via the write attribute:"}, {"cell_type": "markdown", "metadata": {}, "source": "We can use the `write` attribute to write it to file:"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/plain": "<pyspark.sql.readwriter.DataFrameWriter at 0x7f90c049df40>"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "df.write"}, {"cell_type": "markdown", "metadata": {}, "source": "After we have a DataFrameWriter, we specify three values: the format, a series of options, and the save mode. At a minimum, you must supply a path. We will cover the potential for options, which vary from data source to data source, shortly."}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "df.write.format(\"csv\")\\\n  .option(\"header\", \"True\")\\\n  .mode(\"overwrite\")\\\n  .save(\"path/to/file(s)\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### SAVE MODES\nSave modes specify what will happen if Spark finds data at the specified location (assuming all else equal). The following table lists the save modes:\n\n| Save mode | Description |\n| -- | -- |\n| append | Appends the output files to the list of files that already exist at that location |\n| overwrite | Will completely overwrite any data that already exists there |\n| errorIfExists | Throws an error and fails the write if data or files already exist at the specified location |\n| ignore | If data or files exist at the location, do nothing with the current DataFrame |\n\nThe default is `errorIfExists`. This means that if Spark finds data at the location to which you\u2019re writing, it will fail the write immediately.\n\nWe\u2019ve largely covered the core concepts that you\u2019re going to need when using data sources, so now let\u2019s dive into each of Spark\u2019s native data sources.\n\n### CSV Files\nCSV stands for commma-separated values. This is a common text file format in which each line represents a single record, and commas separate each field within a record. CSV files, while seeming well structured, are actually one of the trickiest file formats you will encounter because not many assumptions can be made in production scenarios about what they contain or how they are structured. For this reason, the CSV reader has a large number of options. These options give you the ability to work around issues like certain characters needing to be escaped\u2014for example, commas inside of columns when the file is also comma-delimited or null values labeled in an unconventional way.\n\n### CSV Options\nFollowing table presents the options available in the CSV reader:\n\n\n|Read/write|Key|Potential values|Default |Description|\n|--|--|--|--|--|\n|Both|sep|Any single string character|,|The single character that is used as separator for each field and value.|\n|Both|header|true, false|false|A Boolean flag that declares whether the first line in the file(s) are the names of the columns.|\n|Read|escape|Any string character|\"\\\\\"|The character Spark should use to escape other characters in the file.|\n|Read|inferSchema|true, false|false|Specifies whether Spark should infer column types when reading the file.|\n|Read|ignoreLeadingWhiteSpace|true, false|false|Declares whether leading spaces from values being read should be skipped.|\n|Read|ignoreTrailingWhiteSpace|true, false|false|Declares whether trailing spaces from values being read should be skipped.|\n|Both|nullValue|Any string character|\u201c\u201d|Declares what character represents a null value in the file.|\n|Both|nanValue|Any string character|NaN|Declares what character represents a NaN or missing character in the CSV file.|\n|Both|positiveInf|Any string or character|Inf|Declares what character(s) represent a positive infinite value.|\n|Both|negativeInf|Any string or character|-Inf|Declares what character(s) represent a negative infinite value.|\n|Both|compression or codec|None, uncompressed, bzip2, deflate, gzip, lz4, or snappy|none|Declares what compression codec Spark should use to read or write the file.|\n|Both|dateFormat|Any string or character that conforms to java\u2019s SimpleDataFormat.|yyyy-MM-dd|Declares the date format for any columns that are date type.|\n|Both|timestampFormat|Any string or character that conforms to java\u2019s SimpleDataFormat.|yyyy-MM-dd\u2019T\u2019HH:mm:ss.SSSZZ|Declares the timestamp format for any columns that are timestamp type.|\n|Read|maxColumns|Any integer|20480|Declares the maximum number of columns in the file.|\n|Read|maxCharsPerColumn|Any integer|1000000|Declares the maximum number of characters in a column.|\n|Read|escapeQuotes|true, false|true|Declares whether Spark should escape quotes that are found in lines.|\n|Read|maxMalformedLogPerPartition|Any integer|10|Sets the maximum number of malformed rows Spark will log for each partition. Malformed records beyond this number will be ignored.|\n|Write|quoteAll|true, false|false|Specifies whether all values should be enclosed in quotes, as opposed to just escaping values that have a quote character.|\n|Read|multiLine|true, false|false|This option allows you to read multiline CSV files where each logical row in the CSV file might span multiple rows in the file itself.|\n\n### Reading CSV Files\nTo read a CSV file, like any other format, we must first create a `DataFrameReader` for that specific format. Here, we specify the format to be CSV:"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": "<pyspark.sql.readwriter.DataFrameReader at 0x7f90c049d640>"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "spark.read.format(\"csv\")"}, {"cell_type": "markdown", "metadata": {}, "source": "After this, we have the option of specifying a schema as well as modes as options. Let\u2019s set a couple of options, some that we have seen above and others that we haven\u2019t seen yet. We\u2019ll set the `header` to `true` for our CSV file, the `mode` to be `FAILFAST`, and `inferSchema` to `true`:"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"mode\", \"FAILFAST\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(data + \"flight-data/csv/2010-summary.csv\")\n# FAILFAST: Fails immediately upon encountering malformed records"}, {"cell_type": "markdown", "metadata": {}, "source": "As mentioned, we can use the mode to specify how much tolerance we have for malformed data. For example, we can use these modes and a manual schema to ensure that our file(s) conform to the data that we expected:"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"}], "source": "from pyspark.sql.types import StructField, StructType, StringType, LongType\n\nmyManualSchema = StructType([\n  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n  StructField(\"count\", LongType(), True)\n])\n\nspark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"mode\", \"FAILFAST\")\\\n  .schema(myManualSchema)\\\n  .load(data + \"flight-data/csv/2010-summary.csv\")\\\n  .show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Writing CSV Files\n\nJust as with reading data, there are a variety of options (listed in the table above) for writing data when we write CSV files. This is a subset of the reading options because many do not apply when writing data (like maxColumns and inferSchema). Here\u2019s an example:"}, {"cell_type": "markdown", "metadata": {}, "source": "Let's first use a filter to create a small DataFrame from our loaded df:"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"}], "source": "df.show(5)"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-----+\n|DEST_COUNTRY_NAME|count|\n+-----------------+-----+\n|           Canada| 8271|\n|           Mexico| 6200|\n|   United Kingdom| 1629|\n|          Germany| 1392|\n|            Japan| 1383|\n+-----------------+-----+\nonly showing top 5 rows\n\n"}], "source": "from pyspark.sql.functions import desc, col\noutbound_US_2010 = df.where(\"ORIGIN_COUNTRY_NAME == 'United States'\")\\\n  .where(\"DEST_COUNTRY_NAME <> 'United States'\")\\\n  .select(\"DEST_COUNTRY_NAME\", \"count\")\\\n  .orderBy(col(\"count\").desc())\n\noutbound_US_2010.show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "We will write this DataFrame to a TSV file in our Google Cloud bucket. Before writing let's check out the current physical plan for our DataFrame `outbound_US_2010`:"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Sort [count#18 DESC NULLS LAST], true, 0\n   +- Exchange rangepartitioning(count#18 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#117]\n      +- Project [DEST_COUNTRY_NAME#16, count#18]\n         +- Filter (((isnotnull(ORIGIN_COUNTRY_NAME#17) AND isnotnull(DEST_COUNTRY_NAME#16)) AND (ORIGIN_COUNTRY_NAME#17 = United States)) AND NOT (DEST_COUNTRY_NAME#16 = United States))\n            +- FileScan csv [DEST_COUNTRY_NAME#16,ORIGIN_COUNTRY_NAME#17,count#18] Batched: false, DataFilters: [isnotnull(ORIGIN_COUNTRY_NAME#17), isnotnull(DEST_COUNTRY_NAME#16), (ORIGIN_COUNTRY_NAME#17 = Un..., Format: CSV, Location: InMemoryFileIndex[gs://is843/notebooks/jupyter/data/flight-data/csv/2010-summary.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ORIGIN_COUNTRY_NAME), IsNotNull(DEST_COUNTRY_NAME), EqualTo(ORIGIN_COUNTRY_NAME,United..., ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n\n\n"}], "source": "outbound_US_2010.explain()"}, {"cell_type": "markdown", "metadata": {}, "source": "Since the default number of partitions for shuffling is 200 if we write the current DataFrame into a file it will result in 200 small files (this happened after the orderBy operation). In fact since this is a small DataFrame it will be 124 files since we have 124 rows in the DataFrame. This is less than ideal as it will add significant time to read/write. We can fix this by resetting `shuffle.partitions` value:"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "spark.conf.set('spark.sql.shuffle.partitions', '1')"}, {"cell_type": "markdown", "metadata": {}, "source": "If we try now we will end up with only 1 file:"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "outbound_US_2010 DataFrame was written to a CSV file in the following path: gs://is843/notebooks/jupyter/data/flight-data/csv/outbound_US_2010.tsv\n"}], "source": "outbound_US_2010.write.format(\"csv\")\\\n  .option(\"header\", \"True\")\\\n  .option(\"sep\", \"\\t\")\\\n  .mode(\"overwrite\")\\\n  .save(data + \"tmp/outbound_US_2010.tsv\")\n\nprint(\"outbound_US_2010 DataFrame was written to a CSV file in the following path: {}flight-data/csv/outbound_US_2010.tsv\".format(data))"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"data": {"text/plain": "'gs://is843/notebooks/jupyter/data/tmp/outbound_US_2010.tsv'"}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": "data + \"tmp/outbound_US_2010.tsv\""}, {"cell_type": "markdown", "metadata": {}, "source": "### Writing to Hadoop File System\nWe can also write this file in our cluster's Hadoop File System:"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "outbound_US_2010.write.format(\"csv\")\\\n  .option(\"header\", \"True\")\\\n  .option(\"sep\", \"\\t\")\\\n  .mode(\"overwrite\")\\\n  .save(\"/tmp/outbound_US_2010.tsv\")"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 2 items\n-rw-r--r--   1 root hadoop          0 2022-03-29 22:16 /tmp/outbound_US_2010.tsv/_SUCCESS\n-rw-r--r--   1 root hadoop       1727 2022-03-29 22:16 /tmp/outbound_US_2010.tsv/part-00000-e7cd6c3a-e50d-48f9-87cd-4fb50969488b-c000.csv\n"}], "source": "!hadoop fs -ls /tmp/outbound_US_2010.tsv"}, {"cell_type": "markdown", "metadata": {}, "source": "The number of files or data written is dependent on the number of partitions the DataFrame has at the time you write out the data. By default, one file is written per partition of the data. This means that although we specify a \u201cfile\u201d, it\u2019s actually a number of files within a folder, with the name of the specified file, with one file per each partition that is written.\n\noutbound_US_2010.tsv is actually a folder that contains our CSV file(s). This actually reflects the number of partitions in our DataFrame at the time we write it out. If we were to repartition our data before then, we would end up with a different number of files. Let's look at the first few rows of this file and confirm that the format is as what we expected.\n\nUse the following command to check the first few rows. Replace FILENAME with the name of the file from the result of the list above:\n\n```bash\n!hadoop fs -cat FILENAME | head\n```"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "DEST_COUNTRY_NAME\tcount\nCanada\t8271\nMexico\t6200\nUnited Kingdom\t1629\nGermany\t1392\nJapan\t1383\nDominican Republic\t1109\nBrazil\t995\nThe Bahamas\t903\nColombia\t785\n"}], "source": "# Your code goes here\n!hadoop fs -cat /tmp/outbound_US_2010.tsv/* | head"}, {"cell_type": "markdown", "metadata": {}, "source": "## JSON Files\nRefer to chapter 9 for the specifics of read/write of this type of files.\n\n## Parquet Files\nParquet is an open source column-oriented data store that provides a variety of storage optimizations, especially for analytics workloads. It provides columnar compression, which saves storage space and allows for reading individual columns instead of entire files. It is a file format that works exceptionally well with Apache Spark and is in fact the default file format. We recommend writing data out to Parquet for long-term storage because reading from a Parquet file will always be more efficient than JSON or CSV. Another advantage of Parquet is that it supports complex types. This means that if your column is an array (which would fail with a CSV file, for example), map, or struct, you\u2019ll still be able to read and write that file without issue. Here\u2019s how to specify Parquet as the read format:"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"data": {"text/plain": "<pyspark.sql.readwriter.DataFrameReader at 0x7f90c0307940>"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "spark.read.format(\"parquet\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### Reading Parquet Files\nParquet has very few options because it enforces its own schema when storing data. Thus, all you need to set is the format and you are good to go. We can set the schema if we have strict requirements for what our DataFrame should look like. Oftentimes this is not necessary because we can use schema on read, which is similar to the inferSchema with CSV files. However, with Parquet files, this method is more powerful because the schema is built into the file itself (so no inference needed).\n\nHere are some simple examples reading from parquet:"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 17:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|    1|\n|    United States|            Ireland|  264|\n|    United States|              India|   69|\n|            Egypt|      United States|   24|\n|Equatorial Guinea|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.read.format(\"parquet\").load(data + \"flight-data/parquet/2010-summary.parquet\").show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "### PARQUET OPTIONS\nAs we just mentioned, there are very few Parquet options\u2014precisely two, in fact\u2014because it has a well-defined specification that aligns closely with the concepts in Spark. Table below presents the options:\n\n|Read/Write|Key|Potential Values|Default|Description|\n|--|--|--|--|--|\n|Write|compression or codec|None, uncompressed, bzip2, deflate, gzip, lz4, or snappy|None|Declares what compression codec Spark should use to read or write the file.|\n|Read|mergeSchema|true, false|Value of the configuration spark.sql.parquet.mergeSchema|You can incrementally add columns to newly written Parquet files in the same table/folder. Use this option to enable or disable this feature.|\n\nWriting Parquet Files\nWriting Parquet is as easy as reading it. We simply specify the location for the file. The same partitioning rules apply:"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "outbound_US_2010.write.format(\"parquet\").mode(\"overwrite\")\\\n  .save(data + \"tmp/outbound_US_2010.parquet\") "}, {"cell_type": "markdown", "metadata": {}, "source": "We can easily read back this file into a DataFrame:"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-----+\n|DEST_COUNTRY_NAME|count|\n+-----------------+-----+\n|           Canada| 8271|\n|           Mexico| 6200|\n|   United Kingdom| 1629|\n|          Germany| 1392|\n|            Japan| 1383|\n+-----------------+-----+\nonly showing top 5 rows\n\n"}], "source": "spark.read.format(\"parquet\").load(data + \"tmp/outbound_US_2010.parquet\").show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "## BigQuery\n\n### Note: At the time of writing this notebook and with the current settings this sample code is not working. Please refer to the [documentation](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example) and explore the potential solutions.\n\nThe BigQuery connector can be used with Apache Spark to read and write data from/to BigQuery. This section provides an example code that uses the BigQuery connector with PySpark.\n\nUsing the following commands we can obtain the bucket name and project name for the cluster (this way we don't have to manually modify it):"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "is843\nis843-demo\n"}], "source": "bucket = spark._jsc.hadoopConfiguration().get(\"fs.gs.system.bucket\")\nproject = spark._jsc.hadoopConfiguration().get(\"fs.gs.project.id\")\n\nprint(bucket)\nprint(project)"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "spark.conf.set('temporaryGcsBucket', bucket)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Load data from BigQuery.\nwords = spark.read.format('bigquery') \\\n  .option('table', 'bigquery-public-data:samples.shakespeare') \\\n  .load()\nwords.createOrReplaceTempView('words')\n\n# Perform word count.\nword_count = spark.sql(\n    'SELECT word, SUM(word_count) AS word_count FROM words GROUP BY word')\nword_count.show()\nword_count.printSchema()\n\n# Saving the data to BigQuery\nword_count.write.format('bigquery') \\\n  .option('table', 'wordcount_dataset.wordcount_output') \\\n  .save()"}, {"cell_type": "markdown", "metadata": {}, "source": "The following code will get the information to bring in the BigQuery table `bigquery-public-data.samples.shakespeare` into Spark.\n\nIt will first save the BigQuery table as a JSON object in Google Cloud Storage and then we convert this object into a DataFrame:"}, {"cell_type": "markdown", "metadata": {}, "source": "\\* Notice that you will get an error if you run the above cell more than once, since the files already exist in the tmp folder that we have specified."}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+---------------+\n|              corpus|sum(word_count)|\n+--------------------+---------------+\n|              hamlet|          32446|\n|      kingrichardiii|          31868|\n|          coriolanus|          29535|\n|           cymbeline|          29231|\n|        2kinghenryiv|          28241|\n|             othello|          28076|\n|            kinglear|          28001|\n|          kinghenryv|          27894|\n|  troilusandcressida|          27837|\n|  antonyandcleopatra|          27310|\n|        2kinghenryvi|          27165|\n|        1kinghenryiv|          26401|\n|        3kinghenryvi|          26288|\n|       kinghenryviii|          26265|\n|      romeoandjuliet|          26192|\n|         winterstale|          26181|\n|allswellthatendswell|          24622|\n|       kingrichardii|          24150|\n| merrywivesofwindsor|          24033|\n|  measureforemeasure|          23303|\n|        1kinghenryvi|          23272|\n|    loveslabourslost|          23189|\n|         asyoulikeit|          23087|\n| muchadoaboutnothing|          22760|\n|    merchantofvenice|          22448|\n|    tamingoftheshrew|          22358|\n|            kingjohn|          21983|\n|     titusandronicus|          21911|\n|        twelfthnight|          21633|\n|        juliuscaesar|          21052|\n|periclesprinceoftyre|          19846|\n|       timonofathens|          19841|\n|twogentlemenofverona|          18493|\n|             macbeth|          18439|\n|             sonnets|          17805|\n|             tempest|          17593|\n|midsummersnightsd...|          17348|\n|      comedyoferrors|          16361|\n|       rapeoflucrece|          15221|\n|      venusandadonis|          10035|\n|             various|           3545|\n|     loverscomplaint|           2586|\n+--------------------+---------------+\n\n"}], "source": "from pyspark.sql import functions as F\n\n# Casting word_count as integer\nword_count = word_count.withColumn(\"word_count\", F.col(\"word_count\").cast(\"INT\"))\n\nword_count.groupBy(\"corpus\").sum(\"word_count\").orderBy(F.desc(\"sum(word_count)\")).show(50)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Partitioning\nPartitioning is a tool that allows you to control what data is stored (and where) as you write it. When you write a file to a partitioned directory (or table), you basically encode a column as a folder. What this allows you to do is skip lots of data when you go to read it in later, allowing you to read in only the data relevant to your problem instead of having to scan the complete dataset. These are supported for all file-based data sources:"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": "df.write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\\\n  .save(\"/tmp/partitioned-files.parquet\")"}, {"cell_type": "markdown", "metadata": {}, "source": "Upon writing, you get a list of folders in your Parquet \u201cfile\u201d:"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "19/03/26 03:09:24 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: 1.7.0-hadoop2\nFound 126 items\ndrwxr-xr-x   - root hadoop          0 2019-03-26 03:09 /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Afghanistan\ndrwxr-xr-x   - root hadoop          0 2019-03-26 03:09 /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Angola\ndrwxr-xr-x   - root hadoop          0 2019-03-26 03:09 /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Anguilla\ndrwxr-xr-x   - root hadoop          0 2019-03-26 03:09 /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Antigua and Barbuda\ndrwxr-xr-x   - root hadoop          0 2019-03-26 03:09 /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Argentina\ndrwxr-xr-x   - root hadoop          0 2019-03-26 03:09 /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Aruba\ndrwxr-xr-x   - root hadoop          0 2019-03-26 03:09 /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Australia\ndrwxr-xr-x   - root hadoop          0 2019-03-26 03:09 /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Austria\ndrwxr-xr-x   - root hadoop          0 2019-03-26 03:09 /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Azerbaijan\n"}], "source": "!hadoop fs -ls /tmp/partitioned-files.parquet | head"}, {"cell_type": "markdown", "metadata": {}, "source": "Each of these will contain Parquet files that contain that data where the previous predicate was true:"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "19/03/26 03:09:26 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: 1.7.0-hadoop2\nFound 1 items\n-rw-r--r--   2 root hadoop        646 2019-03-26 03:09 /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Afghanistan/part-00000-492e9e7c-30f4-4f73-b8fd-a783025ae7c9.c000.snappy.parquet\n"}], "source": "!hadoop fs -ls /tmp/partitioned-files.parquet/DEST_COUNTRY_NAME=Afghanistan"}, {"cell_type": "markdown", "metadata": {}, "source": "This is probably the lowest-hanging optimization that you can use when you have a table that readers frequently filter by before manipulating. For instance, date is particularly common for a partition because, downstream, often we want to look at only the previous week\u2019s data (instead of scanning the entire list of records). This can provide massive speedups for readers.\n\n## Managing File Size\nManaging file sizes is an important factor not so much for writing data but reading it later on. When you\u2019re writing lots of small files, there\u2019s a significant metadata overhead that you incur managing all of those files. Spark especially does not do well with small files, although many file systems (like HDFS) don\u2019t handle lots of small files well, either. You might hear this referred to as the \u201csmall file problem.\u201d The opposite is also true: you don\u2019t want files that are too large either, because it becomes inefficient to have to read entire blocks of data when you need only a few rows.\n\nSpark 2.2 introduced a new method for controlling file sizes in a more automatic way. We saw previously that the number of output files is a derivative of the number of partitions we had at write time (and the partitioning columns we selected). Now, you can take advantage of another tool in order to limit output file sizes so that you can target an optimum file size. You can use the *maxRecordsPerFile* option and specify a number of your choosing. This allows you to better control file sizes by controlling the number of records that are written to each file. For example, if you set an option for a writer as `df.write.option(\"maxRecordsPerFile\", 5000)`, Spark will ensure that files will contain at most 5,000 records."}, {"cell_type": "markdown", "metadata": {}, "source": "## SQL Databases (JDBC Connection)\n\nSQL datasources are one of the more powerful connectors because there are a variety of systems to which you can connect (as long as that system speaks SQL). For instance you can connect to a MySQL database, a PostgreSQL database, or an Oracle database. Despite the importance of these kinds of data sources the details of JDBC connections are beyond the scope of this course. If interested more details can be found in chapter 9 and Spark documentation."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 4}