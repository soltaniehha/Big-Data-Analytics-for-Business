{"cells": [{"cell_type": "markdown", "metadata": {"id": "HmKczPYqu3n7"}, "source": "# Structured Streaming Basics\n\nStructured Streaming is a stream processing framework built on the Spark SQL engine. Rather than introducing a separate API, Structured Streaming uses the existing structured APIs in Spark (DataFrames, Datasets, and SQL), meaning that all the operations you are familiar with there are supported. Users express a streaming computation in the same way they\u2019d write a batch computation on static data. Upon specifying this, and specifying a streaming destination, the Structured Streaming engine will take care of running your query incrementally and continuously as new data arrives into the system. These logical instructions for the computation are then executed using the same Catalyst engine, including query optimization, code generation, etc. Beyond the core structured processing engine, Structured Streaming includes a number of features specifically for streaming. For instance, Structured Streaming ensures end-to-end, exactly-once processing as well as fault-tolerance through checkpointing and write-ahead logs.\n\nThe main idea behind Structured Streaming is to treat a stream of data as a table to which data is continuously appended. The job then periodically checks for new input data, process it, updates some internal state located in a state store if needed, and updates its result. A cornerstone of the API is that you should not have to change your query\u2019s code when doing batch or stream processing\u2014you should have to specify only whether to run that query in a batch or streaming fashion. Internally, Structured Streaming will automatically figure out how to \u201cincrementalize\u201d your query, i.e., update its result efficiently whenever new data arrives, and will run it in a fault-tolerant fashion.\n\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/13-02-streaming.png?raw=true\" width=\"800\" align=\"center\"/>"}, {"cell_type": "markdown", "metadata": {"id": "9zxvFzQju3n9"}, "source": "In simplest terms, Structured Streaming is \u201cyour DataFrame, but streaming.\u201d This makes it very easy to get started using streaming applications. You probably already have the code for them! There are some limits to the types of queries Structured Streaming will be able to run, however, as well as some new concepts you have to think about that are specific to streaming, such as event-time and out-of-order data.\n\nYou can use Structured Streaming to continuously update a table that users query interactively with Spark SQL, serve a machine learning model trained by MLlib, or join streams with offline data in any of Spark\u2019s data sources\u2014applications that would be much more complex to build using a mix of different tools.\n\n## Core Concepts\n\nNow that we introduced the high-level idea, let\u2019s cover some of the important concepts in a Structured Streaming job. One thing you will hopefully find is that there aren\u2019t many. That\u2019s because Structured Streaming is designed to be simple. Read some other big data streaming books and you\u2019ll notice that they begin by introducing terminology like distributed stream processing topologies for skewed data reducers (a caricature, but accurate) and other complex verbiage. Spark\u2019s goal is to handle these concerns automatically and give users a simple way to run any Spark computation on a stream.\n\n### Transformations and Actions\n\nStructured Streaming maintains the same concept of transformations and actions that we have seen throughout this book. The transformations available in Structured Streaming are, with a few restrictions, the exact same transformations that we saw before. The restrictions usually involve some types of queries that the engine cannot incrementalize yet, although some of the limitations are being lifted in new versions of Spark. There is generally only one action available in Structured Streaming: that of starting a stream, which will then run continuously and output results."}, {"cell_type": "markdown", "metadata": {"id": "8cEOAmtSu3n9"}, "source": "### Input Sources\n\nStructured Streaming supports the following built-in input sources in Spark 3.5.3:\n\n* File source (HDFS, S3, GCS, ADLS, local)\n* Apache Kafka\n* Rate source\n* Rate-microbatch source\n* Socket source (for testing)\n\nAdditional sources such as Kinesis, Event Hubs, Pub/Sub, Pulsar, Delta, Iceberg, and Hudi \nare supported via external connectors but are not part of core Spark.\n\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/streaming-input-source.png?raw=true\" width=\"800\" align=\"center\"/>\n\nInternally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.\n\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/streaming-batches.png?raw=true\" width=\"800\" align=\"center\"/>"}, {"cell_type": "markdown", "metadata": {"id": "jhm45h1xu3n9"}, "source": "\n### Sinks\n\nJust as sources allow you to get data into Structured Streaming, sinks specify the destination for the result set of that stream. Sinks and the execution engine are also responsible for reliably tracking the exact progress of data processing. Supported Structured Streaming sinks in Spark 3.5.3:\n\n* File sink (Parquet, JSON, ORC, CSV, etc.)\n* Kafka sink\n* Foreach (row-wise) sink\n* ForeachBatch (micro-batch) sink\n* Console sink (testing)\n* Memory sink (debugging)\n\n**Input source:** where data comes from\n\n**Sink:** where processed data goes\n\n### Output Modes\n\nDefining a sink for our Structured Streaming job is only half of the story. We also need to define how we want Spark to write data to that sink. For instance, do we only want to append new information? Do we want to update rows as we receive more information about them over time (e.g., updating the click count for a given web page)? Do we want to completely overwrite the result set every single time (i.e. always write a file with the complete click counts for all pages)? To do this, we define an output mode, similar to how we define output modes in the static Structured APIs.\n\nThe supported output modes are as follows:\n\n* Append (only add new records to the output sink)\n\n* Update (update changed records in place)\n\n* Complete (rewrite the full output)\n\nOne important detail is that certain queries, and certain sinks, only support certain output modes. For example, suppose that your job is just performing a map on a stream. The output data will grow indefinitely as new records arrive, so it would not make sense to use Complete mode, which requires writing all the data to a new file at once. In contrast, if you are doing an aggregation into a limited number of keys, Complete and Update modes would make sense, but Append would not, because the values of some keys need to be updated over time."}, {"cell_type": "markdown", "metadata": {"id": "0QGFqu7xu3n9"}, "source": "### Triggers\n\nWhereas output modes define how data is output, triggers define when data is output\u2014that is, when Structured Streaming should check for new input data and update its result. By default, Structured Streaming will look for new input records as soon as it has finished processing the last group of input data, giving the lowest latency possible for new results. However, this behavior can lead to writing many small output files when the sink is a set of files. Thus, Spark also supports triggers based on processing time (only look for new data at a fixed interval). In the future, other types of triggers may also be supported.\n\n## Structured Streaming in Action\n\nLet\u2019s get to an applied example of how you might use Structured Streaming. For our examples, we\u2019re going to be working with the Heterogeneity Human Activity Recognition Dataset. The data consists of smartphone and smartwatch sensor readings from a variety of devices\u2014specifically, the accelerometer and gyroscope, sampled at the highest possible frequency supported by the devices. Readings from these sensors were recorded while users performed activities like biking, sitting, standing, walking, and so on. There are several different smartphones and smartwatches used, and nine total users.\n\nLet\u2019s read in the **static** version of the dataset as a DataFrame:"}, {"cell_type": "code", "execution_count": 10, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 288, "status": "ok", "timestamp": 1731555245338, "user": {"displayName": "Mohammad Soltanieh Ha", "userId": "12308918870841825745"}, "user_tz": 300}, "id": "YurYFaciu3n-", "outputId": "d5f90856-b1f7-41e7-ede6-7a1616a821ce", "tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "gs://is843-public/data/\n"}], "source": "# This folder has public access\ndata = \"gs://is843-public/data/\"\nprint(data)"}, {"cell_type": "code", "execution_count": 11, "metadata": {"id": "D2t2lydWu3n-", "outputId": "9431643c-3c0d-47ba-a9ce-c617344beb16", "tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- Arrival_Time: long (nullable = true)\n |-- Creation_Time: long (nullable = true)\n |-- Device: string (nullable = true)\n |-- Index: long (nullable = true)\n |-- Model: string (nullable = true)\n |-- User: string (nullable = true)\n |-- gt: string (nullable = true)\n |-- x: double (nullable = true)\n |-- y: double (nullable = true)\n |-- z: double (nullable = true)\n\n+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\nonly showing top 2 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 3:====================================================>    (11 + 1) / 12]\r"}, {"name": "stdout", "output_type": "stream", "text": "There are 6240991 rows.\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "static = spark.read.json(data + \"activity-data/\")\nstatic.printSchema()\nstatic.show(2)\nprint(\"There are {} rows.\".format(static.count()))"}, {"cell_type": "markdown", "metadata": {"id": "-2xlYFZPu3n_"}, "source": "You can see in the preceding example, which includes a number of timestamp columns, models, user, and device information. The `gt` field specifies what activity the user was doing at that time.\n\nNext, let\u2019s create a streaming version of the same Dataset, which will read each input file in the dataset one by one as if it was a stream.\n\nStreaming DataFrames are largely the same as static DataFrames. We create them within Spark applications and then perform transformations on them to get our data into the correct format. Basically, all of the transformations that are available in the static Structured APIs apply to Streaming DataFrames. However, one small difference is that Structured Streaming does not let you perform schema inference without explicitly enabling it. You can enable schema inference for this by setting the configuration spark.sql.streaming.schemaInference to true. Given that fact, we will read the schema from one file (that we know has a valid schema) and pass the dataSchema object from our static DataFrame to our streaming DataFrame. As mentioned, you should avoid doing this in a production scenario where your data may (accidentally) change out from under you:"}, {"cell_type": "code", "execution_count": 12, "metadata": {"id": "nj2Qberku3n_", "tags": []}, "outputs": [], "source": "dataSchema = static.schema"}, {"cell_type": "code", "execution_count": 13, "metadata": {"id": "T1gmw7gTu3n_", "tags": []}, "outputs": [], "source": "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n  .json(data + \"activity-data/\")"}, {"cell_type": "markdown", "metadata": {"id": "evwFbNY8u3n_"}, "source": "`maxFilesPerTrigger` essentially it allows you to control how quickly Spark will read all of the files in the folder. By specifying this value lower, we\u2019re artificially limiting the flow of the stream to one file per trigger. This helps us demonstrate how Structured Streaming runs incrementally in our example, but probably isn\u2019t something you\u2019d use in production."}, {"cell_type": "markdown", "metadata": {"id": "45JKOaFWu3n_"}, "source": "Just like with other Spark APIs, streaming DataFrame creation and execution is lazy. In particular, we can now specify transformations on our streaming DataFrame before finally calling an action to start the stream. In this case, we\u2019ll show one simple transformation\u2014we will group and count data by the `gt` column, which is the activity being performed by the user at that point in time:"}, {"cell_type": "code", "execution_count": 14, "metadata": {"id": "n2yX3f8Lu3n_", "tags": []}, "outputs": [], "source": "activityCounts = streaming.groupBy(\"gt\").count()"}, {"cell_type": "markdown", "metadata": {"id": "7SovV9iVu3n_"}, "source": "Because this code is being written in a small cluster, we are going to set the shuffle partitions to a small value to avoid creating too many shuffle partitions:"}, {"cell_type": "code", "execution_count": 15, "metadata": {"id": "9-a3e9AOu3n_", "tags": []}, "outputs": [], "source": "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"}, {"cell_type": "markdown", "metadata": {"id": "MquKggJHu3n_"}, "source": "Now that we set up our transformation, we need only to specify our action to start the query. We will specify an output destination, or output sink for our result of this query. For this basic example, we are going to write to a memory sink which keeps an in-memory table of the results.\n\nIn the process of specifying this sink, we\u2019re going to need to define how Spark will output that data. In this example, we use the complete output mode. This mode rewrites all of the keys along with their counts after every trigger:"}, {"cell_type": "code", "execution_count": 20, "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/11/19 22:03:35 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n[Stage 14:=============================>                            (2 + 2) / 4]\r"}], "source": "activityQuery = (\n    activityCounts.writeStream\n    .queryName(\"activity_counts\")\n    .format(\"memory\")\n    .outputMode(\"complete\")\n    .option(\"checkpointLocation\", \"/home/jupyter/spark_checkpoints/activity_counts\")\n    .start()\n)"}, {"cell_type": "markdown", "metadata": {"id": "sSZkEEwru3oA"}, "source": "We are now writing out our stream! You\u2019ll notice that we set a unique query name to represent this stream, in this case activity_counts. We specified our format as an in-memory table and we set the output mode.\n\nWhen we run the preceding code, we also want to include the following line:\n\n```python\nactivityQuery.awaitTermination()\n```"}, {"cell_type": "markdown", "metadata": {"id": "Is41fNLzu3oA"}, "source": "After this code is executed, the streaming computation will have started in the background. The query object is a handle to that active streaming query, and we must specify that we would like to wait for the termination of the query using activityQuery.awaitTermination() to prevent the driver process from exiting while the query is active. We will omit this for now, but it must be included in your production applications; otherwise, your stream won\u2019t be able to run.\n\nSpark lists this stream, and other active ones, under the active streams in our SparkSession. We can see a list of those streams by running the following:"}, {"cell_type": "code", "execution_count": 21, "metadata": {"id": "UWyUgWPwu3oA", "outputId": "f56866ca-5ff7-4db9-81d3-36e54bd89839", "tags": []}, "outputs": [{"data": {"text/plain": "[<pyspark.sql.streaming.query.StreamingQuery at 0x7fda7aeaad90>]"}, "execution_count": 21, "metadata": {}, "output_type": "execute_result"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.streams.active"}, {"cell_type": "markdown", "metadata": {"id": "3EkY860Bu3oA"}, "source": "Now that this stream is running, we can experiment with the results by querying the in-memory table it is maintaining of the current output of our streaming aggregation. This table will be called activity_counts, the same as the stream. To see the current data in this output table, we simply need to query it! We\u2019ll do this in a simple loop that will print the results of the streaming query every second:"}, {"cell_type": "code", "execution_count": 22, "metadata": {"id": "vSRsISV3u3oA", "outputId": "6bb6b050-e1a8-444f-fd29-6b29587f19c0", "tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------+------+\n|        gt| count|\n+----------+------+\n|       sit|123085|\n|     stand|113849|\n|stairsdown| 93648|\n|      walk|132560|\n|  stairsup|104521|\n|      null|104482|\n|      bike|107974|\n+----------+------+\n\n+----------+------+\n|        gt| count|\n+----------+------+\n|       sit|135392|\n|     stand|125234|\n|stairsdown|103010|\n|      walk|145816|\n|  stairsup|114975|\n|      null|114931|\n|      bike|118773|\n+----------+------+\n\n+----------+------+\n|        gt| count|\n+----------+------+\n|       sit|160006|\n|     stand|148005|\n|stairsdown|121732|\n|      walk|172327|\n|  stairsup|135888|\n|      null|135827|\n|      bike|140370|\n+----------+------+\n\n+----------+------+\n|        gt| count|\n+----------+------+\n|       sit|184620|\n|     stand|170778|\n|stairsdown|140456|\n|      walk|198839|\n|  stairsup|156800|\n|      null|156721|\n|      bike|161965|\n+----------+------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+------+\n|        gt| count|\n+----------+------+\n|       sit|196927|\n|     stand|182165|\n|stairsdown|149819|\n|      walk|212095|\n|  stairsup|167255|\n|      null|167168|\n|      bike|172762|\n+----------+------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from time import sleep\nfor x in range(5):\n    spark.sql(\"SELECT * FROM activity_counts\").show()\n    sleep(2)"}, {"cell_type": "markdown", "metadata": {"id": "3OlgFZH4u3oA"}, "source": "With this simple example, the power of Structured Streaming should become clear. You can take the same operations that you use in batch and run them on a stream of data with very few code changes (essentially just specifying that it\u2019s a stream)."}], "metadata": {"colab": {"provenance": []}, "kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 4}